{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "231415ff",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa857766",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import glob\n",
    "from karateclub import DeepWalk\n",
    "from scipy.stats import entropy\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56776ad5",
   "metadata": {},
   "source": [
    "### Create heterogenous graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5105412",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The code in this cell was inspired by the following resource(s):\n",
    "# - https://www.geeksforgeeks.org/create-heterogeneous-graph-using-dgl-in-python/\n",
    "# - https://pytorch-geometric.readthedocs.io/en/latest/notes/heterogeneous.html\n",
    "\n",
    "# Create a heterogeneous graph using networkx\n",
    "G = nx.Graph()\n",
    "\n",
    "# Add nodes with different types\n",
    "num_authors = 10\n",
    "num_papers = 15\n",
    "num_conferences = 5\n",
    "\n",
    "for i in range(1, num_authors + 1):\n",
    "    G.add_node(f\"author{i}\", type=\"author\")\n",
    "\n",
    "for i in range(1, num_papers + 1):\n",
    "    G.add_node(f\"paper{i}\", type=\"paper\")\n",
    "\n",
    "for i in range(1, num_conferences + 1):\n",
    "    G.add_node(f\"conference{i}\", type=\"conference\")\n",
    "\n",
    "# Add edges between different types of nodes with edge types\n",
    "# Each author writes 3 random papers\n",
    "for i in range(1, num_authors + 1):\n",
    "    authored_papers = np.random.choice(range(1, num_papers + 1), size=3, replace=False)\n",
    "    for paper_id in authored_papers:\n",
    "        G.add_edge(f\"author{i}\", f\"paper{paper_id}\", type=\"writes\")\n",
    "\n",
    "# Each paper is presented at 1 random conference\n",
    "for paper_id in range(1, num_papers + 1):\n",
    "    conference_id = np.random.randint(1, num_conferences + 1)\n",
    "    G.add_edge(f\"paper{paper_id}\", f\"conference{conference_id}\", type=\"presented_at\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bed2dd8",
   "metadata": {},
   "source": [
    "### Plot the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca911cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The code in this cell was inspired by the following resource(s):\n",
    "# - https://matplotlib.org/stable/api/_as_gen/matplotlib.lines.Line2D.html\n",
    "\n",
    "# Function to plot graphs with node types in different colors and edge types in different styles\n",
    "def plot_graph(graph, pos, title):\n",
    "    plt.figure(figsize=(12, 10))\n",
    "\n",
    "    # Define colors for nodes based on their types\n",
    "    node_colors = []\n",
    "    for node in graph.nodes():\n",
    "        if graph.nodes[node]['type'] == 'author':\n",
    "            node_colors.append('lightblue')\n",
    "        elif graph.nodes[node]['type'] == 'paper':\n",
    "            node_colors.append('lightgreen')\n",
    "        elif graph.nodes[node]['type'] == 'conference':\n",
    "            node_colors.append('lightcoral')\n",
    "\n",
    "    # Define edge styles based on their types\n",
    "    edge_colors = []\n",
    "    edge_styles = []\n",
    "    for edge in graph.edges:\n",
    "        if graph.edges[edge]['type'] == 'presented_at':\n",
    "            edge_colors.append('gray')\n",
    "            edge_styles.append('dashed')\n",
    "        elif graph.edges[edge]['type'] == 'writes':\n",
    "            edge_colors.append('gray')\n",
    "            edge_styles.append('solid')\n",
    "        else:\n",
    "            edge_colors.append('gray')\n",
    "            edge_styles.append('solid')\n",
    "\n",
    "    # Draw nodes and edges with specified colors and styles\n",
    "    nx.draw(graph, pos, with_labels=True, node_color=node_colors, edge_color=edge_colors, style=edge_styles,\n",
    "            node_size=400, font_size=15, width=2)  # Set the width parameter for thicker edges\n",
    "\n",
    "    # Create legend for node types\n",
    "    author_node = plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='lightblue', markersize=15, label='Author')\n",
    "    paper_node = plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='lightgreen', markersize=15, label='Paper')\n",
    "    conference_node = plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='lightcoral', markersize=15, label='Conference')\n",
    "\n",
    "    # Create legend for edge types\n",
    "    writes_edge = plt.Line2D([0], [0], color='gray', linewidth=2, linestyle='-', label='Writes')\n",
    "    presented_at_edge = plt.Line2D([0], [0], color='gray', linewidth=2, linestyle='--', label='Presented at')\n",
    "\n",
    "    plt.legend(handles=[author_node, paper_node, conference_node, writes_edge, presented_at_edge], loc='upper right', fontsize=15)\n",
    "\n",
    "    plt.title(title, fontsize=35)\n",
    "    plt.show()\n",
    "\n",
    "# nx.write_gml(G, 'graph_tests_data/graph_test_30_nodes.gml') # Save the graph\n",
    "\n",
    "G = nx.read_gml('graph_tests_data/graph_test_30_nodes.gml') # Read the graph\n",
    "\n",
    "# Plot the original graph\n",
    "pos = nx.spring_layout(G, seed=42, k=0.3)  # Spring layout with adjusted k for better spacing\n",
    "plot_graph(G, pos, 'Original Graph')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b59f38",
   "metadata": {},
   "source": [
    "### Apply DeepWalk to get node embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80058618",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The code in this cell was inspired by the following resource(s):\n",
    "# - https://medium.com/@taunkdhaval08/graph-representational-learning-creating-node-and-graph-embeddings-part-2-33817c5ce7f3\n",
    "\n",
    "# Create a mapping from original nodes to consecutive integers\n",
    "node_mapping = {node: idx for idx, node in enumerate(G.nodes())}\n",
    "reverse_mapping = {idx: node for node, idx in node_mapping.items()}\n",
    "\n",
    "# Create a new graph with the mapped node indices\n",
    "G_mapped = nx.Graph()\n",
    "for node in G.nodes():\n",
    "    G_mapped.add_node(node_mapping[node])\n",
    "for edge in G.edges():\n",
    "    G_mapped.add_edge(node_mapping[edge[0]], node_mapping[edge[1]])\n",
    "\n",
    "# Fit the DeepWalk model\n",
    "model = DeepWalk(dimensions=64, walk_length=30, workers=4)\n",
    "model.fit(G_mapped)\n",
    "\n",
    "# Get the embeddings\n",
    "embeddings = model.get_embedding()\n",
    "\n",
    "# Print node embeddings with original node names\n",
    "for idx, embedding in enumerate(embeddings):\n",
    "    original_node = reverse_mapping[idx]\n",
    "    print(f\"Node: {original_node}, Embedding: {embedding}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4201533",
   "metadata": {},
   "source": [
    "### Apply PCA to reduce dimensionality of node embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b360bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The code in this cell was inspired by the following resource(s):\n",
    "# - https://builtin.com/machine-learning/pca-in-python\n",
    "\n",
    "# Reduce dimensionality of embeddings for visualization\n",
    "pca = PCA(n_components=2)\n",
    "reduced_embeddings = pca.fit_transform(embeddings)\n",
    "\n",
    "# Print reduced embeddings\n",
    "for node, embedding in zip(G.nodes(), reduced_embeddings):\n",
    "    print(f\"Node: {node}, Reduced Embedding: {embedding}\")\n",
    "\n",
    "# Define colors for each node type\n",
    "color_map = {'author': 'lightblue', 'paper': 'lightgreen', 'conference': 'lightcoral'}\n",
    "node_colors = [color_map[G.nodes[node]['type']] for node in G.nodes()]\n",
    "\n",
    "# Plot the reduced embeddings with larger points\n",
    "plt.figure(figsize=(10, 10))\n",
    "for i, node in enumerate(G.nodes()):\n",
    "    plt.scatter(reduced_embeddings[i, 0], reduced_embeddings[i, 1], color=node_colors[i], s=400)  # Adjust the value of `s` to change the size\n",
    "    plt.annotate(node, (reduced_embeddings[i, 0], reduced_embeddings[i, 1]), fontsize=10)\n",
    "\n",
    "# Create custom legend\n",
    "legend_elements = [\n",
    "    plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='lightblue', markersize=15, label='Author'),\n",
    "    plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='lightgreen', markersize=15, label='Paper'),\n",
    "    plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='lightcoral', markersize=15, label='Conference')\n",
    "]\n",
    "\n",
    "plt.legend(handles=legend_elements, loc='upper right', fontsize=15)\n",
    "\n",
    "plt.title(\"Node Embeddings by Type\", fontsize=20)\n",
    "plt.xlabel(\"PCA Component 1\", fontsize=15)\n",
    "plt.ylabel(\"PCA Component 2\", fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83bbbcea",
   "metadata": {},
   "source": [
    "### Cluster reduced node embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f62588e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The code in this cell was inspired by the following resource(s):\n",
    "# - https://www.w3schools.com/python/python_ml_k-means.asp\n",
    "\n",
    "# Apply K-Means clustering\n",
    "num_clusters = 3  # Define the number of clusters\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=0).fit(reduced_embeddings)\n",
    "clusters = kmeans.labels_\n",
    "\n",
    "# Print clusters\n",
    "for node, cluster in zip(G.nodes(), clusters):\n",
    "    print(f\"Node: {node}, Cluster: {cluster}\")\n",
    "\n",
    "# Define a color map for clusters\n",
    "color_map = plt.get_cmap('viridis', num_clusters)\n",
    "\n",
    "# Plot the reduced embeddings with cluster labels\n",
    "plt.figure(figsize=(12, 12))\n",
    "for i, node in enumerate(G.nodes()):\n",
    "    plt.scatter(reduced_embeddings[i, 0], reduced_embeddings[i, 1], color=color_map(clusters[i]), s=400, alpha=0.6, edgecolors='w')\n",
    "    plt.annotate(node, (reduced_embeddings[i, 0], reduced_embeddings[i, 1]), fontsize=12, alpha=0.75)\n",
    "\n",
    "plt.title(\"Node Embeddings with Clusters\", fontsize=25)\n",
    "plt.colorbar(plt.cm.ScalarMappable(cmap=color_map), ticks=range(num_clusters), label='Cluster')\n",
    "plt.grid(True)\n",
    "plt.tick_params(axis='both', which='major', labelsize=13)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf08ad9",
   "metadata": {},
   "source": [
    "### Identify central nodes using centrality measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6776ef3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate centrality measures\n",
    "degree_centrality = nx.degree_centrality(G)\n",
    "betweenness_centrality = nx.betweenness_centrality(G)\n",
    "eigenvector_centrality = nx.eigenvector_centrality(G, max_iter=1000)\n",
    "\n",
    "# Print centrality measures\n",
    "print(\"\\n######### Degree Centrality: #########\")\n",
    "for node, centrality in degree_centrality.items():\n",
    "    print(f\"Node: {node}, Centrality: {centrality}\")\n",
    "\n",
    "print(\"\\n######### Betweenness Centrality: #########\")\n",
    "for node, centrality in betweenness_centrality.items():\n",
    "    print(f\"Node: {node}, Centrality: {centrality}\")\n",
    "\n",
    "print(\"\\n######### Eigenvector Centrality: #########\")\n",
    "for node, centrality in eigenvector_centrality.items():\n",
    "    print(f\"Node: {node}, Centrality: {centrality}\")\n",
    "\n",
    "# Identify central nodes in each cluster using different centrality measures\n",
    "central_nodes = {i: {} for i in range(num_clusters)}\n",
    "centrality_measures = {\n",
    "    'degree': degree_centrality,\n",
    "    'betweenness': betweenness_centrality,\n",
    "    'eigenvector': eigenvector_centrality\n",
    "}\n",
    "\n",
    "for measure_name, centrality in centrality_measures.items():\n",
    "    for i in range(num_clusters):\n",
    "        cluster_nodes = [node for node, cluster in zip(G.nodes(), clusters) if cluster == i]\n",
    "        if not cluster_nodes:\n",
    "            print(f\"Cluster {i} is empty for {measure_name} centrality\")\n",
    "            continue\n",
    "        cluster_centralities = {node: centrality[node] for node in cluster_nodes}\n",
    "        sorted_cluster_nodes = sorted(cluster_centralities, key=cluster_centralities.get, reverse=True)\n",
    "        central_nodes[i][measure_name] = sorted_cluster_nodes[:10]  # Top 10 central nodes in each cluster\n",
    "\n",
    "# Print central nodes\n",
    "for cluster, nodes in central_nodes.items():\n",
    "    print(f\"\\nCluster {cluster}:\")\n",
    "    for measure, top_nodes in nodes.items():\n",
    "        print(f\"  {measure.capitalize()} Centrality: {top_nodes}\")\n",
    "        \n",
    "# Initialize an empty set to collect all identified nodes\n",
    "identified_nodes = set()\n",
    "\n",
    "# Iterate over central_nodes dictionary and collect nodes into the set\n",
    "for cluster, nodes in central_nodes.items():\n",
    "    for measure, top_nodes in nodes.items():\n",
    "        identified_nodes.update(top_nodes)\n",
    "\n",
    "# Convert the set to a sorted list\n",
    "identified_nodes_list = sorted(list(identified_nodes))\n",
    "\n",
    "# Print the list of identified nodes\n",
    "print(\"List of Identified Nodes:\")\n",
    "for node in identified_nodes_list:\n",
    "    print(node)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d0e8e6",
   "metadata": {},
   "source": [
    "### Compute edge embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e581d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The code in this cell was inspired by the following resource(s):\n",
    "# - https://github.com/benedekrozemberczki/M-NMF\n",
    "\n",
    "# Compute adjacency matrix\n",
    "adj_matrix = nx.to_numpy_array(G)\n",
    "\n",
    "# Perform NMF on adjacency matrix\n",
    "nmf = NMF(n_components=64, random_state=42)\n",
    "W = nmf.fit_transform(adj_matrix)\n",
    "H = nmf.components_\n",
    "\n",
    "# Standardize W and H\n",
    "W = StandardScaler().fit_transform(W)\n",
    "H = StandardScaler().fit_transform(H.T)\n",
    "\n",
    "# Compute edge embeddings using matrix factorization\n",
    "edge_embeddings_nmf = {}\n",
    "for edge in G.edges():\n",
    "    u, v = edge\n",
    "    u_idx = list(G.nodes()).index(u)\n",
    "    v_idx = list(G.nodes()).index(v)\n",
    "    embedding = np.concatenate([W[u_idx], H[v_idx]])\n",
    "    edge_embeddings_nmf[edge] = embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7cd56b6",
   "metadata": {},
   "source": [
    "### Apply PCA to reduce dimensionality of edge embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286f6876",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce dimensionality of embeddings for visualization\n",
    "edge_embeddings_nmf_array = np.array(list(edge_embeddings_nmf.values()))\n",
    "pca = PCA(n_components=2)\n",
    "reduced_edge_embeddings_nmf = pca.fit_transform(edge_embeddings_nmf_array)\n",
    "\n",
    "# Print reduced edge embeddings (NMF)\n",
    "print(\"Edge embeddings from NMF:\")\n",
    "for edge, embedding in zip(edge_embeddings_nmf.keys(), reduced_edge_embeddings_nmf):\n",
    "    print(f\"Edge: {edge}, Reduced Embedding: {embedding}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf637026",
   "metadata": {},
   "source": [
    "### Measure edge importance using Non-Negative Matrix Factorisation (NMF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bac098c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure edge influence for NMF\n",
    "edge_influence_nmf = {edge: np.linalg.norm(embedding) for edge, embedding in edge_embeddings_nmf.items()}\n",
    "\n",
    "# Sort edges by influence measure (NMF)\n",
    "sorted_edges_nmf = sorted(edge_influence_nmf.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Visualize the graph with influential edges highlighted (NMF)\n",
    "plt.figure(figsize=(12, 12))\n",
    "\n",
    "# Adjusting node and edge properties for better visualization\n",
    "node_colors = 'lightblue'\n",
    "edge_colors_nmf = [edge_influence_nmf[edge] for edge in G.edges()]\n",
    "edge_widths_nmf = [edge_influence_nmf[edge] * 0.5 for edge in G.edges()]  # Adjust edge width multiplier here\n",
    "\n",
    "# Draw nodes and edges with thinner widths\n",
    "nx.draw_networkx_nodes(G, pos, node_color=node_colors, node_size=300)\n",
    "nx.draw_networkx_edges(G, pos, edgelist=edge_influence_nmf.keys(), edge_color=edge_colors_nmf,\n",
    "                       width=edge_widths_nmf, edge_cmap=plt.cm.Blues, alpha=0.7)\n",
    "\n",
    "# Add labels to nodes\n",
    "nx.draw_networkx_labels(G, pos, font_size=7, font_color='black')\n",
    "\n",
    "# Add color bar for edge influences\n",
    "sm = plt.cm.ScalarMappable(cmap=plt.cm.Blues, norm=plt.Normalize(vmin=min(edge_influence_nmf.values()), vmax=max(edge_influence_nmf.values())))\n",
    "sm.set_array([])\n",
    "plt.colorbar(sm, label='Edge Influence')\n",
    "\n",
    "plt.title(\"Graph with Influential Edges Highlighted (NMF)\", fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6e1842",
   "metadata": {},
   "source": [
    "### Create sampled subgraph using central nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea63bf26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the node types and their colors\n",
    "node_type_color_map = {\n",
    "    'author': 'lightblue',\n",
    "    'paper': 'lightgreen',\n",
    "    'conference': 'lightcoral'\n",
    "}\n",
    "\n",
    "# Example of setting node types in the graph (if not already set)\n",
    "for node in G.nodes():\n",
    "    if 'author' in node:\n",
    "        G.nodes[node]['type'] = 'author'\n",
    "    elif 'paper' in node:\n",
    "        G.nodes[node]['type'] = 'paper'\n",
    "    elif 'conference' in node:\n",
    "        G.nodes[node]['type'] = 'conference'\n",
    "        \n",
    "# Create a subgraph with only the key important nodes\n",
    "subgraph = G.subgraph(identified_nodes_list).copy()\n",
    "\n",
    "# Extend the subgraph by including neighbors of the key nodes\n",
    "extended_nodes = set(identified_nodes_list)\n",
    "for node in identified_nodes_list:\n",
    "    extended_nodes.update(G.neighbors(node))\n",
    "\n",
    "# Create an extended subgraph\n",
    "extended_subgraph = G.subgraph(extended_nodes).copy()\n",
    "\n",
    "# Remove isolated nodes (nodes with no edges) from the extended subgraph\n",
    "isolated_nodes = [node for node in extended_subgraph.nodes if extended_subgraph.degree(node) == 0]\n",
    "extended_subgraph.remove_nodes_from(isolated_nodes)\n",
    "\n",
    "# Function to plot graphs with node types in different colors and edge types in different styles\n",
    "def plot_graph(graph, pos, title):\n",
    "    plt.figure(figsize=(12, 10))\n",
    "\n",
    "    # Define colors for nodes based on their types\n",
    "    node_colors = []\n",
    "    for node in graph.nodes():\n",
    "        if graph.nodes[node]['type'] == 'author':\n",
    "            node_colors.append('lightblue')\n",
    "        elif graph.nodes[node]['type'] == 'paper':\n",
    "            node_colors.append('lightgreen')\n",
    "        elif graph.nodes[node]['type'] == 'conference':\n",
    "            node_colors.append('lightcoral')\n",
    "\n",
    "    # Define edge styles based on their types\n",
    "    edge_styles = [('dashed' if graph.edges[edge]['type'] == 'presented_at' else 'solid' if graph.edges[edge]['type'] == 'writes' else 'solid') for edge in graph.edges]\n",
    "\n",
    "    # Draw nodes and edges with specified colors and styles\n",
    "    nx.draw(graph, pos, with_labels=True, node_color=node_colors, edge_color='gray', style=edge_styles,\n",
    "            node_size=300, font_size=10, width=3)\n",
    "\n",
    "    # Create legend for node types\n",
    "    author_node = plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='lightblue', markersize=15, label='Author')\n",
    "    paper_node = plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='lightgreen', markersize=15, label='Paper')\n",
    "    conference_node = plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='lightcoral', markersize=15, label='Conference')\n",
    "\n",
    "    # Create legend for edge types\n",
    "    writes_edge = plt.Line2D([0], [0], color='gray', linewidth=3, linestyle='-', label='Writes')\n",
    "    presented_at_edge = plt.Line2D([0], [0], color='gray', linewidth=3, linestyle='--', label='Presented at')\n",
    "\n",
    "    plt.legend(handles=[author_node, paper_node, conference_node, writes_edge, presented_at_edge], loc='upper right', fontsize=15)\n",
    "\n",
    "    plt.title(title, fontsize=35)\n",
    "    plt.show()\n",
    "    \n",
    "# Print the number of nodes in the graph\n",
    "num_nodes = extended_subgraph.number_of_nodes()\n",
    "print(f\"Number of nodes in the graph: {num_nodes}\")\n",
    "\n",
    "# Plot the graph\n",
    "pos = nx.spring_layout(extended_subgraph, seed=42, k=0.3)  # Spring layout with adjusted k for better spacing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04288c48",
   "metadata": {},
   "source": [
    "### Create subgraph using both central nodes and important edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67065a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute adjacency matrix for the extended subgraph\n",
    "adj_matrix = nx.to_numpy_array(extended_subgraph)\n",
    "\n",
    "# Perform NMF on adjacency matrix\n",
    "nmf = NMF(n_components=64, random_state=42)\n",
    "W = nmf.fit_transform(adj_matrix)\n",
    "H = nmf.components_\n",
    "\n",
    "# Standardize W and H\n",
    "W = StandardScaler().fit_transform(W)\n",
    "H = StandardScaler().fit_transform(H.T)\n",
    "\n",
    "# Compute edge embeddings using matrix factorization\n",
    "edge_embeddings_nmf = {}\n",
    "for edge in extended_subgraph.edges():\n",
    "    u, v = edge\n",
    "    u_idx = list(extended_subgraph.nodes()).index(u)\n",
    "    v_idx = list(extended_subgraph.nodes()).index(v)\n",
    "    embedding = np.concatenate([W[u_idx], H[v_idx]])\n",
    "    edge_embeddings_nmf[edge] = embedding\n",
    "\n",
    "# Measure edge influence for NMF\n",
    "edge_influence_nmf = {edge: np.linalg.norm(embedding) for edge, embedding in edge_embeddings_nmf.items()}\n",
    "\n",
    "# Define a threshold for important edges (keep top 60% most influential edges)\n",
    "threshold = np.percentile(list(edge_influence_nmf.values()), 40)\n",
    "important_edges = [edge for edge, influence in edge_influence_nmf.items() if influence >= threshold]\n",
    "\n",
    "# Filter the extended subgraph to include only important edges\n",
    "important_subgraph = extended_subgraph.edge_subgraph(important_edges).copy()\n",
    "\n",
    "# Remove isolated nodes (nodes with no edges) from the important subgraph\n",
    "isolated_nodes = [node for node in important_subgraph.nodes if important_subgraph.degree(node) == 0]\n",
    "important_subgraph.remove_nodes_from(isolated_nodes)\n",
    "\n",
    "# Visualize the cleaned extended subgraph with important edges\n",
    "plt.figure(figsize=(12, 12))\n",
    "pos = nx.spring_layout(important_subgraph)  # Choose a layout for the graph\n",
    "\n",
    "# Get node colors based on their types\n",
    "node_colors = [node_type_color_map[important_subgraph.nodes[node]['type']] for node in important_subgraph.nodes]\n",
    "\n",
    "# Print the number of nodes\n",
    "num_nodes = important_subgraph.number_of_nodes()\n",
    "print(f\"Number of nodes in the graph important_subgraph: {num_nodes}\")\n",
    "\n",
    "# Create an edge style map based on edge types\n",
    "edge_styles = [('dashed' if important_subgraph.edges[edge]['type'] == 'presented_at' else 'solid' if important_subgraph.edges[edge]['type'] == 'writes' else 'solid') for edge in important_subgraph.edges]\n",
    "\n",
    "# Plot the important subgraph\n",
    "plot_graph(important_subgraph, pos, \"Pruned subgraph\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9dea44",
   "metadata": {},
   "source": [
    "### Evaluate pruning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749a910a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_graph_statistics(original_graph, sampled_graph):\n",
    "    statistics = {}\n",
    "\n",
    "    # Number of Nodes\n",
    "    original_num_nodes = len(original_graph.nodes())\n",
    "    sampled_num_nodes = len(sampled_graph.nodes())\n",
    "    statistics['Number of nodes'] = (original_num_nodes, sampled_num_nodes)\n",
    "\n",
    "    # Number of Edges\n",
    "    original_num_edges = len(original_graph.edges())\n",
    "    sampled_num_edges = len(sampled_graph.edges())\n",
    "    statistics['Number of edges'] = (original_num_edges, sampled_num_edges)\n",
    "\n",
    "    # Average Degree\n",
    "    original_avg_degree = sum(dict(original_graph.degree()).values()) / original_num_nodes\n",
    "    sampled_avg_degree = sum(dict(sampled_graph.degree()).values()) / sampled_num_nodes\n",
    "    statistics['Average node degree'] = (original_avg_degree, sampled_avg_degree)\n",
    "\n",
    "    # Average Clustering Coefficient\n",
    "    original_avg_clustering = nx.average_clustering(original_graph)\n",
    "    sampled_avg_clustering = nx.average_clustering(sampled_graph)\n",
    "    statistics['Average clustering coefficient'] = (original_avg_clustering, sampled_avg_clustering)\n",
    "\n",
    "    # Average Path Length\n",
    "    if nx.is_connected(original_graph) and nx.is_connected(sampled_graph):\n",
    "        original_avg_path_length = nx.average_shortest_path_length(original_graph)\n",
    "        sampled_avg_path_length = nx.average_shortest_path_length(sampled_graph)\n",
    "        statistics['Average path length'] = (original_avg_path_length, sampled_avg_path_length)\n",
    "    else:\n",
    "        statistics['Average path length'] = ('Graph is not connected', 'Graph is not connected')\n",
    "\n",
    "    # Average Betweenness Centrality\n",
    "    original_avg_betweenness = np.mean(list(nx.betweenness_centrality(original_graph).values()))\n",
    "    sampled_avg_betweenness = np.mean(list(nx.betweenness_centrality(sampled_graph).values()))\n",
    "    statistics['Average betweenness centrality'] = (original_avg_betweenness, sampled_avg_betweenness)\n",
    "\n",
    "    # Average Closeness Centrality\n",
    "    original_avg_closeness = np.mean(list(nx.closeness_centrality(original_graph).values()))\n",
    "    sampled_avg_closeness = np.mean(list(nx.closeness_centrality(sampled_graph).values()))\n",
    "    statistics['Average closeness centrality'] = (original_avg_closeness, sampled_avg_closeness)\n",
    "\n",
    "    # Average Degree Centrality\n",
    "    original_avg_degree_centrality = np.mean(list(nx.degree_centrality(original_graph).values()))\n",
    "    sampled_avg_degree_centrality = np.mean(list(nx.degree_centrality(sampled_graph).values()))\n",
    "    statistics['Average degree centrality'] = (original_avg_degree_centrality, sampled_avg_degree_centrality)\n",
    "\n",
    "    # Average Eigenvector Centrality\n",
    "    try:\n",
    "        original_eigenvector_centrality = nx.eigenvector_centrality(original_graph, max_iter=1000)\n",
    "        original_avg_eigenvector_centrality = np.mean(list(original_eigenvector_centrality.values()))\n",
    "    except nx.PowerIterationFailedConvergence:\n",
    "        original_avg_eigenvector_centrality = 'Convergence Failed'\n",
    "\n",
    "    try:\n",
    "        sampled_eigenvector_centrality = nx.eigenvector_centrality(sampled_graph, max_iter=1000)\n",
    "        sampled_avg_eigenvector_centrality = np.mean(list(sampled_eigenvector_centrality.values()))\n",
    "    except nx.PowerIterationFailedConvergence:\n",
    "        sampled_avg_eigenvector_centrality = 'Convergence Failed'\n",
    "\n",
    "    statistics['Average eigenvector centrality'] = (original_avg_eigenvector_centrality, sampled_avg_eigenvector_centrality)\n",
    "\n",
    "    # Density\n",
    "    original_density = nx.density(original_graph)\n",
    "    sampled_density = nx.density(sampled_graph)\n",
    "    statistics['Density'] = (original_density, sampled_density)\n",
    "\n",
    "    return statistics\n",
    "\n",
    "\n",
    "def print_and_collect_statistics_comparison(statistics):\n",
    "    comparison_data = []\n",
    "\n",
    "    for metric, values in statistics.items():\n",
    "        try:\n",
    "            # Convert string values to floats if possible\n",
    "            original_value = float(values[0]) if isinstance(values[0], (int, float)) else values[0]\n",
    "            sampled_value = float(values[1]) if isinstance(values[1], (int, float)) else values[1]\n",
    "            \n",
    "            # Compute the absolute difference if both values are numeric\n",
    "            if isinstance(original_value, (int, float)) and isinstance(sampled_value, (int, float)):\n",
    "                difference = abs(original_value - sampled_value)\n",
    "            else:\n",
    "                difference = 'N/A'\n",
    "        except ValueError:\n",
    "            # Handle non-numeric values separately\n",
    "            difference = 'N/A'\n",
    "        \n",
    "        comparison_data.append([metric, original_value, sampled_value, difference])\n",
    "\n",
    "    return comparison_data\n",
    "\n",
    "# Example usage:\n",
    "# Assuming G and important_subgraph are defined as the original and sampled graphs respectively\n",
    "stats_comparison = evaluate_graph_statistics(G, important_subgraph)\n",
    "comparison_data = print_and_collect_statistics_comparison(stats_comparison)\n",
    "\n",
    "# Create DataFrame from the comparison data\n",
    "df_comparison = pd.DataFrame(comparison_data, columns=['Metric', 'Original', 'Sampled', 'Difference'])\n",
    "print(df_comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef537994",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63ab5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_comparison.to_csv('results99.csv', index=False)  # Set index=False to exclude row numbers in the CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70c6903",
   "metadata": {},
   "source": [
    "### Evaluating model on graph of 30 nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17030416",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read CSV and plot the 'Difference' column for each row\n",
    "def plot_differences():\n",
    "    # Use glob to find all CSV files in the specified directory\n",
    "    csv_files = glob.glob('../results/DeepWalk_csv_results/30_node_graph_csv_results/*.csv') ##EMPTY\n",
    "    csv_files.sort()  # Sort the files to ensure consistent order\n",
    "\n",
    "    # List to store the DataFrames\n",
    "    all_dataframes = []\n",
    "\n",
    "    for file in csv_files:\n",
    "        # Read the CSV file into a DataFrame\n",
    "        df = pd.read_csv(file)\n",
    "        all_dataframes.append(df)\n",
    "\n",
    "    # Get the list of row labels (metrics) from the first DataFrame and remove unwanted metrics\n",
    "    row_labels = all_dataframes[0]['Metric'].tolist()\n",
    "    unwanted_metrics = ['Number of nodes', 'Number of edges', 'Average node degree', 'Average path length']\n",
    "    row_labels = [metric for metric in row_labels if metric not in unwanted_metrics]\n",
    "\n",
    "    # Create a dictionary to store differences for each metric\n",
    "    all_differences = {metric: [] for metric in row_labels}\n",
    "\n",
    "    # Iterate over each DataFrame and populate the differences for each metric\n",
    "    for df in all_dataframes:\n",
    "        for metric in row_labels:\n",
    "            difference = df[df['Metric'] == metric]['Difference'].astype(float).values[0]\n",
    "            all_differences[metric].append(difference)\n",
    "\n",
    "    # Generate x-tick labels as percentages, ending with 99%\n",
    "    num_files = len(csv_files)\n",
    "    percentage_labels = [f'{i*10}%' for i in range(1, num_files)]\n",
    "    percentage_labels.append('99%')\n",
    "\n",
    "    # Ensure percentage_labels has the same length as the number of files\n",
    "    percentage_labels = percentage_labels[:num_files]\n",
    "\n",
    "    # Plot the differences for each metric\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    for metric, differences in all_differences.items():\n",
    "        plt.plot(percentage_labels, differences, marker='o', label=metric, linewidth=2)  # Increased line thickness\n",
    "\n",
    "    # Add a horizontal line at 0.05\n",
    "    threshold = 0.05\n",
    "    plt.axhline(y=threshold, color='black', linestyle='--', linewidth=1.5)\n",
    "\n",
    "    # Find the maximum pruning level where all differences are below 0.05\n",
    "    max_pruning_level = None\n",
    "    for i, diffs in enumerate(zip(*all_differences.values())):\n",
    "        if all(diff < 0.05 for diff in diffs):\n",
    "            max_pruning_level = i\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    # Annotate the plot with the maximum pruning level\n",
    "    if max_pruning_level is not None:\n",
    "        plt.axvline(x=max_pruning_level, color='black', linestyle=':', linewidth=2)\n",
    "        plt.text(max_pruning_level, 0.04, f'Pruning Level: {percentage_labels[max_pruning_level]}',\n",
    "                 verticalalignment='bottom', horizontalalignment='right',\n",
    "                 color='black', fontsize=12, bbox=dict(facecolor='white', edgecolor='black', boxstyle='round,pad=0.5'))\n",
    "\n",
    "    plt.xticks(rotation=90, fontsize=12)  # Increased x-tick size\n",
    "    plt.yticks(fontsize=12)  # Increased y-tick size\n",
    "    plt.xlabel('Edge Pruning Level', fontsize=14)  # Updated x-label size\n",
    "    plt.ylabel('Difference', fontsize=14)  # Updated y-label size\n",
    "    plt.title('Edge Pruning level for 30 node graph', fontsize=20)  # Updated title size\n",
    "    plt.legend(fontsize=13)  # Increased legend size\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Call the function to plot the differences\n",
    "plot_differences()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec60cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read CSV and plot the 'Difference' column for each row\n",
    "def plot_differences():\n",
    "    # Use glob to find all CSV files in the specified directory\n",
    "    csv_files = glob.glob('../results/DeepWalk_csv_results/30_node_graph_csv_results/*.csv')\n",
    "    csv_files.sort()  # Sort the files to ensure consistent order\n",
    "\n",
    "    # List to store the DataFrames\n",
    "    all_dataframes = []\n",
    "\n",
    "    for file in csv_files:\n",
    "        # Read the CSV file into a DataFrame\n",
    "        df = pd.read_csv(file)\n",
    "        all_dataframes.append(df)\n",
    "\n",
    "    # Get the list of row labels (metrics) from the first DataFrame\n",
    "    row_labels = all_dataframes[0]['Metric'].tolist()\n",
    "\n",
    "    # Define the unwanted metrics to plot\n",
    "    unwanted_metrics = ['Number of nodes', 'Number of edges']\n",
    "\n",
    "    # Create a dictionary to store differences for each unwanted metric\n",
    "    all_differences = {metric: [] for metric in unwanted_metrics}\n",
    "\n",
    "    # Iterate over each DataFrame and populate the differences for each metric\n",
    "    for df in all_dataframes:\n",
    "        for metric in unwanted_metrics:\n",
    "            if metric in row_labels:\n",
    "                difference = df[df['Metric'] == metric]['Difference'].astype(float).values[0]\n",
    "                all_differences[metric].append(difference)\n",
    "\n",
    "    # Generate x-tick labels as percentages, ending with 99%\n",
    "    num_files = len(csv_files)\n",
    "    percentage_labels = [f'{i*10}%' for i in range(1, num_files)]\n",
    "    percentage_labels.append('99%')\n",
    "\n",
    "    # Plot the differences for each unwanted metric\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    for metric, differences in all_differences.items():\n",
    "        plt.plot(percentage_labels, differences, marker='o', label=metric, linewidth=2)\n",
    "\n",
    "    plt.xticks(rotation=90, fontsize=12)\n",
    "    plt.yticks(fontsize=12)\n",
    "    plt.xlabel('Edge Pruning Level', fontsize=14)\n",
    "    plt.ylabel('Difference', fontsize=14)\n",
    "    plt.title('Node and Edge Pruning for 30 node graph', fontsize=16)\n",
    "    plt.legend(fontsize=13)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Call the function to plot the differences\n",
    "plot_differences()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd1e154",
   "metadata": {},
   "source": [
    "### Compute entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b187bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The code in this cell was inspired by the following resource(s):\n",
    "# - https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.entropy.html\n",
    "\n",
    "def compute_degree_distribution(graph):\n",
    "    degrees = [d for n, d in graph.degree()]\n",
    "    unique_degrees, counts = np.unique(degrees, return_counts=True)\n",
    "    degree_distribution = counts / counts.sum()\n",
    "    return degree_distribution\n",
    "\n",
    "def graph_entropy(degree_distribution):\n",
    "    return entropy(degree_distribution)\n",
    "\n",
    "def compute_entropy_difference(original_graph, pruned_graph):\n",
    "    # Calculate degree distributions\n",
    "    original_distribution = compute_degree_distribution(original_graph)\n",
    "    pruned_distribution = compute_degree_distribution(pruned_graph)\n",
    "    \n",
    "    # Calculate entropy\n",
    "    original_entropy = graph_entropy(original_distribution)\n",
    "    pruned_entropy = graph_entropy(pruned_distribution)\n",
    "    \n",
    "    # Absolute difference in entropy\n",
    "    entropy_difference = abs(original_entropy - pruned_entropy)\n",
    "    \n",
    "    return original_entropy, pruned_entropy, entropy_difference\n",
    "    \n",
    "original_entropy, pruned_entropy, entropy_diff = compute_entropy_difference(G, important_subgraph)\n",
    "    \n",
    "print(f\"Original Graph Entropy: {original_entropy}\")\n",
    "print(f\"Pruned Graph Entropy: {pruned_entropy}\")\n",
    "print(f\"Entropy Difference: {entropy_diff}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
